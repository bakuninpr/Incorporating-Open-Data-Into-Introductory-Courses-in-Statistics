##############################################################################
# The open source software R is used to construct a regression model for taxi fares in Chicago 
#
#
# R o R Studio for Windows can be downloaded from
# https://cran.r-project.org/bin/windows/base/ or https://www.rstudio.com/products/rstudio/download3/. 
# This code runs with R 3.3.1 since June, 2018. In case of error,
# verify your version of R and the data; changes may have occurred.
# Created by: Roberto Rivera, roberto.rivera30@upr.edu,
# for context see https://www.tandfonline.com/doi/full/10.1080/10691898.2019.1669506?scroll=top&needAccess=true

# WARNING: The original Chicago Taxi ride data is very large: over 16 GB with about 
# 113 million rows and 24 columns.
# There are many ways to download very large data sets. 
# Ideally a database that can be queried should be used. But these 
# do not tend to be free (Google's BigQuery has a 10GB free limit)
# A very simple way is to download the data (https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew/data)
# and then subset it in R.


# NOTE: Each line that starts with "#" R detects as a comment.


# First a function is created to make packages available. Function provided at https://gist.github.com/stevenworthington/3178163
ipak <- function(pkg){
  new.pkg <- pkg[!(pkg %in% installed.packages()[, "Package"])]
  if (length(new.pkg)) 
    install.packages(new.pkg, dependencies = TRUE)
  sapply(pkg, require, character.only = TRUE)
}

# Now we run the function
packages <- c("data.table", "lubridate", "glmnet","broom","ggplot2") # este paquete se usan para cargar la data de data.pr.gov al programa R
ipak(packages) #install or activate packages

setwd("C:/PATH") #set data file directory
dFile<-'Taxi_Trips.csv' #name of file
chunkSize<-10000000


all_d<-fread(dFile,nrows=chunkSize,header=T,sep=",")
# a much slower alternative is to use the following lines 
#transactFile <- 'https://data.cityofchicago.org/api/views/wrvz-psew/rows.csv?accessType=DOWNLOAD'
#chunkSize<-100000 #note this is much smaller than what we used above!
#con<-file(description=dFile,open="r")
#all_d<-read.table(con,nrows=chunkSize,header=T,fill=TRUE,sep=",") #read.table is much slower than fread
#close(con)



all_d$Fare<-as.numeric(gsub('\\$', '', all_d$Fare))
all_d$Tips<-as.numeric(gsub('\\$', '', all_d$Tips)) #model Tips?
all_d<-as.data.frame(all_d)
who<-which(all_d$Fare<20) 
all_d<-all_d[-who,]
who<-which(all_d$Fare>200)
all_d<-all_d[-who,]


#tips.rat<- all_d$Tips/ all_d$Fare #thing is that in most cases either there are no tips or about 15/20% (w few exceptions that ppl give more)
#all_d$tips.rat<-tips.rat
check <-as.POSIXct(strptime(format(all_d[,4], format="%x %X"),"%m/%d/%Y %I:%M:%S %p",tz=""))
all_d$day <- as.factor(weekdays(check))

all_d$timeint<-cut(hour(check), breaks = c(0, 6, 12, 18, 24), include.lowest = TRUE) 


summary(as.factor(unlist(all_d[,16])))

who<-which(all_d[,16]=="Unknown")
all_d<-all_d[-who,]
who<-which(all_d[,16]=="Dispute"|all_d[,16]=="No Charge")
all_d<-all_d[-who,]

who<-which(all_d[,16]=="Pcard"|all_d[,16]=="Prcard") #TAP cards removed
all_d<-all_d[-who,]

who<-which(all_d[,16]=="Mobile") #credit card? unknown so removed
all_d<-all_d[-who,]

data.dup <- all_d[duplicated(all_d) | duplicated(all_d, fromLast = TRUE), ]
dim(data.dup) #no duplicates!

all_d[,16]<-as.factor(unlist(all_d[,16]))

all_d2<-all_d[,c(5,6,9,10,11,12,16,25,26)]
names(all_d2)<-c("Duration","Distance","Pickup.loc","Dropoff.loc","Fare","Tips","Pay.type","Day","Tod")
names(all_d2)
all_d2$Pickup.loc<-as.factor(all_d2$Pickup.loc)
all_d2$Dropoff.loc<-as.factor(all_d2$Dropoff.loc)
who<-which(all_d2$Distance<5|all_d2$Distance>30)
all_d2<-all_d2[-who,]
who<-which(all_d2$Duration<300|all_d2$Duration>7200)
all_d2<-all_d2[-who,]
speed<-(all_d2$Duration/all_d2$Duration)*3600
who<-which(speed>70) #lowest spped is not too slow
all_d2<-all_d2[-who,]
NROW(na.omit(all_d2)) #number of complete cases that will be used for the model
all_d2<-all_d2[complete.cases(all_d2), ]

d10k<-all_d2[1:10000,]
pairs(d10k[,c("Fare","Duration","Distance","Tips")],upper.panel=NULL)
cor(d10k[,c("Fare","Duration","Distance","Tips")],use="complete.obs")

aggregate(all_d2$Fare,list(all_d2$Tod),length)
aggregate(all_d2$Fare,list(all_d2$Tod),mean)
aggregate(all_d2$Fare,list(all_d2$Tod),sd)

aggregate(Distance~Tod*Day,data=all_d2,mean)

#fit<-lm(Tips~Fare+Trip.time+Trip.miles+pickup.loc*pay.type+dropoff.loc*tod+dow*tod,data=all_d2) # ADD taxi company?B  

# It does not make sense to have Tips as a predictor. It is left out
fit_fare<-lm(Fare~Duration+Distance+Tod+Day+Pickup.loc+Pay.type+Dropoff.loc,data=all_d2)
fit_fare2<-lm(Fare~Duration+Distance*Tod+Distance*Day+Pickup.loc+Pay.type+Dropoff.loc+Day*Tod,data=all_d2)


final_lm<-step(fit_fare)
final_lm2<-step(fit_fare2)


all_d3<-all_d2[1:200000,] #so it runs faster
fit_fare2<-lm(Fare~Duration+Distance*Tod+Distance*Day+Pickup.loc+Pay.type+Dropoff.loc+Day*Tod,data=all_d3)
xt<-model.matrix(fit_fare2)
yt=all_d3[,5]
cv<-cv.glmnet(x=xt,y=yt,alpha=1,nlambda=10)#alpha=1 do lasso
plot(cv)
fit_lasso<-glmnet(x=xt,y=yt,alpha=1,lambda=cv$lambda.1se)

aggregate(Tips~Pay.type,data=all_d2,mean)
fit_fare3<-lm(Fare~Duration+Distance,data=all_d2)

fit_fare_l<-lm(log(Fare)~Duration+Distance,data=all_d2)

fit_l2<-update(fit_1,.~.,-tod)
final_lm<-step(fit_l)

#library(broom) #fortify since ggplot2 may deprecate the function
#library(ggplot2)

#create multiplot function
multiplot <- function(..., plotlist = NULL, file, cols = 1, layout = NULL) {
  require(grid)

  plots <- c(list(...), plotlist)

  numPlots = length(plots)

  if (is.null(layout)) {
    layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
                    ncol = cols, nrow = ceiling(numPlots/cols))
  }

  if (numPlots == 1) {
    print(plots[[1]])

  } else {
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))

    for (i in 1:numPlots) {
      matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))

      print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
                                      layout.pos.col = matchidx$col))
    }
  }
}
myfortfit<-fortify(fit_fare3)#fit_fare_l did not help
minifortfit<-myfortfit[1:100000,]
# USING RESIDUALS INSTEAD OF STD RESIDUALS FOR ANOVA. I INTRODUCE THE STD RESID IN 
# THE SIMPLE LINEAR REG CHAPTER AND CAN USE .stdresid from fortify then
# Homoscedasticity assumption
p1<-ggplot(data = minifortfit, aes(x = .fitted, y = .stdresid)) +
  geom_hline(yintercept = 0, colour = "firebrick3") +
  geom_point()+ylab('Standardized Residual')+xlab('Fitted Values')+
  ggtitle('Standardized Residual vs Fitted')+theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, size=7),axis.title=element_text(size=7)) 



# Normality assumption
p2<-ggplot(data = minifortfit, aes(sample = .stdresid)) +
  stat_qq() + ggtitle('Normal Probability Plot')+ #qq plot and normal probability plot are not the same thing
  ylab('Sample Quantiles')+xlab('Theoretical Quantiles')+
  geom_abline(colour = "firebrick3")+theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, size=7),axis.title=element_text(size=7))

  ncl<-nclass.Sturges(minifortfit$.resid)
p3<-ggplot(minifortfit, aes(x=.stdresid)) + geom_histogram(bins=ncl)#qplot(.stdresid, data=myfortfit,geom="histogram")
    p3<-p3+ylab("")+xlab('Standardized Residual')
    p3<-p3+ggtitle('Histogram')+theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, size=7),axis.title=element_text(size=7))

multiplot(p2,p3,p1,cols=2)


